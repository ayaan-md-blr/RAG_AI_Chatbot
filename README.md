# RAG AI Chatbot using LangChain and Streamlit

This project is a **Retrieval-Augmented Generation (RAG)** based question-answering app that allows you to upload `.json` documents, index them using **HuggingFace embeddings and FAISS**, and then query them using a **locally hosted LLaMA model**.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ app.py               # Streamlit frontend for uploading and querying documents
â”œâ”€â”€ ingest.py            # Script to split, embed, and index documents using FAISS
â”œâ”€â”€ rag_chain.py         # Constructs the QA chain using FAISS, HuggingFace embeddings, and LLaMA
â”œâ”€â”€ constants.py         # Configuration constants used across the app
â”œâ”€â”€ data/                # Folder for uploaded JSON files
â”œâ”€â”€ faiss_index/         # FAISS index (auto-created after indexing)
â””â”€â”€ README.md            # Project documentation
```

---

## âš™ï¸ Requirements

Install the required dependencies:

```bash
pip install -r requirements.txt
```

Dependencies include:

- `streamlit`
- `langchain`
- `faiss-cpu`
- `transformers`
- `llama-cpp-python` (or another LLM backend)
- `huggingface_hub`

---

## ğŸ“ Setup Instructions

1. **Run the Streamlit App**

```bash
streamlit run app.py
```

2. **Index Documents**

- Upload one or more `.json` files through the UI
- Click **"Index Files"** to embed and store them using FAISS

3. **Ask Questions**

Once indexing is complete, enter any question related to the uploaded documents.

---

## âš¡ï¸ How It Works

### `ingest.py`

- Loads JSON files
- Splits content into manageable text chunks
- Embeds using HuggingFace models
- Indexes using FAISS and saves to disk

### `rag_chain.py`

- Loads the FAISS index and embedding model
- Sets up a local LLaMA model
- Builds a `RetrievalQA` chain for answering questions

### `app.py`

- Streamlit interface for file upload and interaction
- Invokes `ingest.py` and `rag_chain.py` functionality dynamically
- Displays results directly in the browser

---

## ğŸ” Notes

- For large local models, ensure you have enough RAM (ideally 8â€“16 GB).
- If using GPU acceleration, adjust `LlamaCpp` settings accordingly.
- Always validate the content of `.json` files before uploading.

---
